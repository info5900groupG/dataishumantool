<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Data Analysis is Human Work</title>

    <!-- Bootstrap Core CSS -->
    <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <!-- MetisMenu CSS -->
    <link href="../vendor/metisMenu/metisMenu.min.css" rel="stylesheet">
    <!-- Custom CSS -->
    <link href="../css/style.css" rel="stylesheet">
    <link href="../dist/css/sb-admin-2.css" rel="stylesheet">
    <!-- Morris Charts CSS -->
    <link href="../vendor/morrisjs/morris.css" rel="stylesheet">
    <!-- Custom Fonts -->
    <link href="../vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
</head>

<body>
    <div id="wrapper">
        <!-- Navigation -->
        <nav class="navbar navbar-default navbar-static-top" role="navigation">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="" data-target=".navbar-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <button class="btn btn-xl btn-link btn-circle" id="gg-learn-more">
                    <i class="fa fa-search-plus"></i>
                </button>
                <!-- Project Background -->
                <div class="gg-title-wrap">
                    <div class="row">
                        <div class="col-lg-12">
                            <div class="well gg-hidden" id="gg-bg-well">
                                <h4>Project Background</h4>
                                <p>With the development of machine learning techniques, the analysis of large-scale data sets (LSDS) have been increasingly applied in many research fields outside of computer science, and become the norm <a href="http://ijoc.org/index.php/ijoc/article/view/2160/1160"><i class="fa fa-external-link fa-fw"></i></a>. The results of these applications have been promising despite the challenges of working with varied, incomplete (often referred to as “messy”) data types. One of the core aspects of these challenges is the change of context for machine learning experts applying their skills to new domains. In the data analysis process, apart from computational labor, a great of “human work” is always required. This includes recognizing feasible situations through preliminary data-analytics, framing issues amenable to machine analysis, devising data representations, etc.</p>
                                <p>As most of this human work is trial and error, difficult to articulate, and sub-conscious, data experts may not keep documentation of these actions, or consider them relevant when explaining their data analysis process. Instead, they tend to only report what they consider to be key milestones or meaningful attempts. This is problematic because it ignores the fact that the data can have a host of issues. As described by Lawrence Busch, some of these issues include: “lossiness” (loss of some meaning due to transformation), layering (wrongly assuming all prior data is equally relevant), or standardization (applying techniques that fit into the research field) <a href="http://ijoc.org/index.php/ijoc/article/view/2160/1160"><i class="fa fa-external-link fa-fw"></i></a>.</p>
                                <p>By underreporting the complete data analysis process, experts may be creating a poor understanding of the data analysis for experts from different academic backgrounds. This means that things are known differentially among different actors. Furthermore, it makes it difficult for other researchers to adequately repeat or review. It is also important to consider how this data analysis is being presented because it is often used when making public and private policy decisions <a href="http://ijoc.org/index.php/ijoc/article/view/2160/1160"><i class="fa fa-external-link fa-fw"></i></a>. For example, in her foundational research on representations in work, Lucy Suchman notes that “In the case of many forms of service work, we recognize that the better the work is done, the less visible it is to those who benefit from it” <a href="http://dx.doi.org/10.1145/223248.223263"><i class="fa fa-external-link fa-fw"></i></a>. Modern day data analysis still falls victim to connotations collusion due to the lack of transparency in the process.</p>
                                <p>The motivation of the present project is to better understand the sociological aspects of the everyday work practices of data analytics and to advance data-analytic research and training. In this project, we conduct data analysis with close attention to the aforementioned problems related to human work of data analytics, such that we can conceptualize and represent them by means of human-computer interaction design methodology.</p>
                            </div>
                        </div>
                    </div>
                </div>
                <!-- /.gg-title-wrap -->
                <div class="gg-title-wrap">
                    <div class="row">
                        <div class="col-lg-10">
                            <h2>Data Analysis is Human Work</h2>
                        </div>
                    </div>
                    <!-- /.row -->
                    <div class="row">
                        <div class="col-lg-5">
                            <h4 class="remove-padding">A tool that shows decision-making in data analysis</h4>
                            <ul class="list-inline">
                                <li id="gg-authors"><div>Authors/Developers:</div></li>
                                <br>
                                <li><a href="thevaleriemack.github.io">Val Mack</a>
                                </li>
                                <li><a href="#">Chen Pan</a>
                                </li>
                                <li><a href="#">Dai Siqi</a>
                                </li>
                                <li><a href="#">Zhenyi Xia</a>
                                </li>
                                <br>
                                <li id="gg-authors"><div>Advisor:</div>
                                </li>
                                <br>
                                <li><a href="#">Samir Passi</a>
                                </li>
                            </ul>
                        </div>
                        <div class="col-lg-7">
                            <div class="gg-intro-message">
                                <p>When we talk about data analysis, we generally refer to the four major components of the process: data collection, processing, analysis, and results. However, <b>there is a lot of unnoticed work that happens in between</b>. This is the work that can not be done by the computer such as judgement-based decision making, quality evaluations, and tasks performed based on prior experiences or outside knowledge. We call this “human work”.</p>
                                <p>This tool is a proof-of-concept that uses the analysis of Reddit Global News headlines and DJIA stock prices as an example for how the human work of data analysis can be shown.
                                </p>
                            </div>
                        </div>
                    </div>
                    <!-- /.row -->
                </div>
                <!-- /.gg-title-wrap -->
            </div>
            <!-- /.navbar-header -->
            <!-- Navbar Right
            <ul class="nav navbar-top-links navbar-right">
                <!-- view as 
                <li class="dropdown">
                    <a class="dropdown-toggle" data-toggle="dropdown" href="#">
                        <i class="fa fa-eye fa-fw"></i> <i class="fa fa-caret-down"></i>
                    </a>
                    <ul class="dropdown-menu dropdown-messages">
                        <li class="gg-list-header"><div><strong>View as</strong></div></li>
                        <li>
                            <a href="timeline.html">
                                <div>
                                    <i class="fa fa-clock-o fa-fw"></i> Timeline
                                </div>
                            </a>
                        </li>
                        <li class="active">
                            <a href="presentation.html">
                                <div>
                                    <i class="fa fa-gift fa-fw"></i> Presentation
                                    <span class="pull-right small">ON</span>
                                </div>
                            </a>
                        </li>
                    </ul>
                </li>
                 /.dropdown -->
                <!-- authors/developers
                <li class="dropdown">
                    <a class="dropdown-toggle" data-toggle="dropdown" href="#">
                        <i class="fa fa-user fa-fw"></i> <i class="fa fa-caret-down"></i>
                    </a>
                    <ul class="dropdown-menu dropdown-user">
                      <li id="gg-authors"><div><strong>Authors/Developers</strong></div></li>
                        <li><a href="thevaleriemack.github.io">Val Mack</a>
                        </li>
                        <li><a href="#">Chen Pan</a>
                        </li>
                        <li><a href="#">Dai Siqi</a>
                        </li>
                        <li><a href="#">Zhenyi Xia</a>
                        </li>
                        <li class="divider"></li>
                        <li><a href="#"><strong>Advisor:</strong> Samir Passi</a>
                        </li>
                    </ul>
                </li>
                <!-- /.dropdown 
            </ul>
              /.navbar-top-links -->
        </nav>
        <!-- /.navbar -->
        <!-- Main Content -->
        <div id="page-wrapper">
            <!-- Abstract -->
            <div class="row gg-row-truncate">
                <div class="col-lg-12">
                    <h3>Abstract</h3>
                    <p>This project explores the human work of data analysis and aims provide a better understanding of the sociological aspects of everyday work practices in data analytics. We conduct our own data analysis on Reddit World News headlines and Dow Jones Industrial Average stock prices. Three forms of feature selection were used with logistic regression to predict the stock index trend. The final result was a 52% prediction accuracy. While proceeding from data collection to results, we tracked every single human decision and action with fine detail. The log of this data was then evaluated and used to inform the present proof-of-concept design. Following this project, the next step is to create a dynamic web page that collects and displays the data analysis process as the analyst does their daily work. Such a tool will improve the fit between technical tools and solutions to real world problems.
                    </p>
                </div>
            </div>
            <!-- /.row -->
            <!-- Tabs -->
            <div class="row">
                <div class="col-lg-12">
                    <br>
                    <ul class="nav nav-pills" id="gg-affix-tabs" data-spy="affix" data-offset="355">
                        <li class="active"><a href="#background-pills" data-toggle="tab">Background</a>
                        </li>
                        <li><a href="#collection-pills" data-toggle="tab">Data Collection</a>
                        </li>
                        <li><a href="#processing-pills" data-toggle="tab">Pre-Processing</a>
                        </li>
                        <li class="dropdown">
                            <a href="#analysis-pills" data-toggle="tab">Data Analysis<span id="gg-caret"><i class="fa fa-caret-down fw"></i></span></a>
                            <span class="dropdown-toggle" id="gg-span-dropdown"></span>
                            <ul class="dropdown-menu" id="gg-ul-dropdown">
                                <li><a class="smoothJump" href="#s1t1">Step 1 Trial 1</a></li>
                                <li><a class="smoothJump" href="#s1t2">Step 1 Trial 2</a></li>
                                <li><a class="smoothJump" href="#s2t1">Step 2 Trial 1</a></li>
                            </ul>
                            <!-- /.dropdown-menu -->
                        </li>
                        <!-- /.dropdown -->
                        <li><a href="#results-pills" data-toggle="tab">Results</a>
                        </li>
                        <li><a href="#discussion-pills" data-toggle="tab">Discussion</a>
                        </li>
                    </ul>
<!--                    <div id="brick"></div>-->
                    <br>
                </div>
            </div>
            <!-- /.row -->
            <!-- Tab Content -->
            <div class="tab-content">
                <!-- Background -->
                <div class="tab-pane active" id="background-pills">
                    <div class="row gg-row-truncate">
                        <div class="col-lg-12">
                            <h4>Background</h4>
                            <p>Predicting stock price movement helps businesses estimate their long-term health and profitability, as well as make the best decisions on future projects. It also provides an opportunity for investors and stock brokers to make strategic portfolio decisions, like whether to short or long a stock, in hopes of generating greater financial returns. For these reasons, numerous studies have tried to identify ways in which stock predictions can be made, in the face of market irrationality.</p>
                            <p>Public and legally acceptable information that would influence movement of a particular stock is generally limited to company press releases, third-party news about companies and their financial status, and current events that may affect a company or industry. (We will refer to these sources as “all available information”.) Because of this, a large subset of natural language research has developed to examine the potential for predictive models of stock price, based on the qualitative and text-based content within these sources. In 1996, Costantino, Collingham, and Morgan, extracted qualitative information to produce templates that provided a meta-analysis of the effects of news on price behavior [5]. This was one of the first examples of employing Natural Language Processing (NLP) in finance. A more recent work by Pablo Azar used sentiment analysis to study the relationship between numerical data from financial markets and verbal information from financial news [2]. This study was based on the “efficient market hypothesis” which claims that there is rarely disagreement about market returns because they affect the true value of products and services. Therefore positive returns are good, and negative returns are bad.</p>
                            <p>Other approaches have drawn strongly from behavioral economics, which claims that besides all available information, human emotions that come from behavioral bias affect stock prices [1]. For example, a 2011 study entitled “Twitter mood predicts the stock market” investigated whether collective mood states from Twitter feeds were correlated to the value of the Dow Jones Industrial Average (DJIA). Researchers analyzed the text content of daily Twitter feeds with the use of two mood tracking tools: OpinionFinder which measures positive vs. negative mood, and Google-Profile of Mood States which employs a 6-dimensional mood measurement. Their model predicted the daily up and down changes in DJIA closing values with 86.7% accuracy, and these results indicated that the accuracy of DJIA predictions can be significantly improved by the inclusion of certain public mood dimensions [3].</p>
                            <p>The present research takes the perspective that sentiment analysis may be more effective when applied to a larger set of information that affects markets on a large scale. For this reason we focus on global news reports and how global stock prices are influenced by them. Now that we have two datasets about news and stock, then we can apply nlp (natural language processing) method to news data and combine the two datasets to do machine learning train. By logistical model we can get a prediction algorithm that tell whether the stock market close price would increase or not if we input the corresponding news data to it.</p>
                        </div>
                    </div>
                    <div class="row">
                                <div class="col-lg-8">
                                    <div class="panel panel-default">
                                        <div class="panel-body text-center">
                                            <img src="../images/mindmap.png" class="img-responsive img-rounded" alt="Mind Map" />
                                        </div>
                                                <!-- /.panel-body -->
                                                <div class="panel-footer">
                                                    <div class="caption text-center small">Mind map of research approach</div>
                                                </div>
                                            </div>
                                    <!-- /.panel -->
                                </div>
                            </div>
                </div>
                <!-- /#background-pills -->
                <!-- Collection -->
                <div class="tab-pane" id="collection-pills">
                    <div class="row gg-row-truncate">
                        <div class="col-lg-12">
                            <h4>
                            Data Collection</h4>
                            <p>The first data set used in this research is the Dow Jones Industrial Average (DJIA). The data originally came from Yahoo Finance and was then compiled into a table showing stock information from August 8, 2008 to July 1, 2016. Features include: the price that the stock opened; the highest, lowest, and closing price for that day; the day’s volume; the adjusted closing price.</p>
                            <!-- stock -->
                            <div class="row">
                                <div class="col-lg-8">
                                    <div class="panel panel-default">
                                        <div class="panel-heading">
                                            <a href="https://www.kaggle.com/aaron7sun/stocknews"><i class="fa fa-external-link fa-fw"></i>Dow Jones Industrial Average (DJIA)
                                            </a>
                                        </div>
                                        <!-- /.panel-heading -->
                                        <div class="panel-body" style="overflow: hidden">
                                            <iframe width="100%" height="400" frameborder="0" scrolling="no" src="//plot.ly/~valem/28.embed">
                                            </iframe>
                                        </div>
                                        <!-- /.panel-body -->
                                        <div class="panel-footer">
                                            <div class="small text-center">Segment of the Yahoo Finance DJIA data set downloaded from Kaggle</div>
                                        </div>
                                    </div>
                                    <!-- /.panel -->
                                </div>
                            </div>
                            <p>One of the notable aspects of this data set is the fact that stocks close on the weekends. Therefore, there is no data between the closing price on Friday and the opening price on the following Monday. These missing values were important to consider, particularly for an analysis based on changing values over time.</p>
                            <p>The second data set is RedditNews. Features of this data set include the date, label, headline, and ranking. It compiles new headlines from the Reddit World News Channel for the aforementioned date range (August 8, 2008 to July 1, 2016) with a label that represents the DJIA stock price movement. "1" means DJIA Adj Close value rose or stayed as the same. "0" means DJIA Adj Close value decreased. The headlines are ranked by how many votes they received from reddit users. In this case, the news headlines from the weekends are omitted.</p>
                            <!-- news -->
                            <div class="row">
                                <div class="col-lg-12">
                                    <div class="panel panel-default">
                                        <div class="panel-heading">
                                            <a href="https://www.kaggle.com/aaron7sun/stocknews"><i class="fa fa-external-link fa-fw"></i>RedditNews
                                            </a>
                                        </div>
                                        <!-- /.panel-heading -->
                                        <div class="panel-body" style="overflow: hidden">
                                            <iframe width="100%" height="600" frameborder="0" scrolling="no" src="//plot.ly/~valem/30.embed"></iframe>
                                        </div>
                                        <!-- /.panel-body -->
                                        <div class="panel-footer">
                                            <div class="small text-center">Segment of the Combined News DJIA data set downloaded from Kaggle</div>
                                        </div>
                                    </div>
                                    <!-- /.panel -->
                                </div>
                            </div>
                            <p>We found these two data sets to be a good selection for this research because they have a large amount of data, ranging 8 years, and different data types. Because one data set is numerical and the other text-based, we hypothesized that the analysis would require different types of methods that would provide a worthwhile challenge. Further, the data reflects daily life well, therefore the model we would create to analyze it may have real world uses beyond the bounds of this research.</p>
                        </div>
                        <!-- /.col-lg-12 -->
                    </div>
                    <!-- /.row -->
                </div>
                <!-- /#collection-pills -->
                <div class="tab-pane" id="processing-pills"></div>
                <!-- /#processing-pills -->
                <!-- Analysis -->
                <div class="tab-pane" id="analysis-pills">
                    <div class="row">
                        <div class="col-lg-12">
                            <h4>Data Analysis</h4>
                            <p>We began by computing the polarity score of combined news headlines, in order to determine the negativity or positivity of a given day. Examination of the results displayed in the table below indicated that there may have been a flaw in our approach, because the compound scores representing a day's polarity were all very close and all negative. We discovered that the Vader module used in the computation was designed for individual sentences, therefore we decided to compute scores for individual headlines instead.</p>
                            <p>At this point, the data was used in a training set for the logistic regression model. There were a total of seven trials using slightly modified inputs in order to produce a result that accurately reflected the data set. The first major change mitigated the missing values for stock price on the weekend by analyzing news polarity data in advance of 1 to 3 days per stock price. The data was then divided such that 80% was used to train the model, and the remaining 20% to test. The final two components were computing the rate of change in polarity to parallel the nature of the binary label that shows stock price movement, and weighting the headlines based on rank.</p>
                        </div>
                    </div>
                    <!-- /.row -->
                    <!-- S1T1 -->
                    <div class="row">
                        <div class="col-lg-12">
                            <h5 id="s1t1">Step 1. Trial 1:</h5>
                            <p>Compute the polarity score of combined news headlines</p>
                            <div class="panel panel-default">
                                <div class="panel-body" style="overflow: scroll">
                                    <iframe width="1000" height="400" frameborder="0" scrolling="no" src="//plot.ly/~valem/32.embed"></iframe>
                                </div>
                                <!-- /.panel-body -->
                                <div class="panel-footer">
                                    <div class="small text-center">Partial view of polarity scores for combined top 25 Reddit World News headlines from 8/8/2008 to 7/1/2016.
                                        <p><strong>Features:</strong> date, label, positive, negative, compound,  neutral <strong>Dimension:</strong> 1990 x 5
                                        </p>
                                    </div>
                                </div>
                                <!-- /.panel-footer -->
                            </div>
                            <!-- /.panel -->
                        </div>
                    </div>
                    <!-- /.row -->
                    <div class="row">
                        <div class="col-lg-12">
                            <div class="well">
                                <h5>Description of Changes</h5>
                                <p>The vader_lexicon.txt corpus from nltk.sentiment.vader module was used to calculate the polarity score of the top 25 headlines combined, resulting in a polarity score that represents the negativity or positivity of each day.</p>
                            </div>
                            <div class="well">
                                <h5>Motivation</h5>
                                <ol>
                                    <li>Based on our hypothesis that the stock market may be affected by the mood in the news, we computed scores to represent the attitude of news headlines.</li>
                                    <li>We tried using the <a href="http://text-processing.com/docs/sentiment.html">text-processing.com Sentiment Analysis API</a> but determined it was not an ideal tool for our purposes.
                                        <ol type="a">
                                          <li>The system produced the result "neutral" for words like "war" and "attack". Which we believed to be negative.</li>
                                          <li>Based on the results, we have reason to believe that all nouns and verbs are considered neutral.</li>
                                          <li>For the aforementioned reasons, we felt it was not a suitable tool to analyze news headlines.</li>
                                        </ol>
                                    </li>
                                    <li>The <a href="#">Vader module</a> returns four features: positive, negative, neutral, and compound. We found it to be appropriately sensitive to the type of content that can be found in news headlines.
                                        <ol type="a">
                                            <li>For example, the headline “Afghan children raped with 'impunity,' U.N. official says - this is sick, a three year old was raped and they do nothing” returned the following values:<br>pos: 0.000000<br>neg: 0.424000<br>compound: -0.926000</li>
                                        </ol>
                                    </li>
                                </ol>
                            </div>
                            <div class="well">
                                <h5>Method</h5>
                                <p>The module was created by using 10 participants to label words on a scale ranging from -4 (extremely negative) to 4 (extremely positive). A neutral word is given a score of 0. Each sentence has three resulting scores: pos_sum, neg_sum, neu_count. pos_sum is the sum of all positively scored words, neg_sum is similarly the score of all negatively scored words, and neu_count is the count of all neutral words in the sentence.</p>
                                Computing each score:
                                <ul>
                                    <li>Positive score: pos_sum / total</li>
                                    <li>Negative score: neg_sum / total</li>
                                    <li>Neutral score: neu_count / total</li>
                                </ul>
                                <p>A total score sums the three individual scores above. A compound score is then calculated by normalizing the total score, putting the result on a scale ranging from -1 (extremely negative) to 1 (extremely positive).</p>
                            </div>
                        </div>
                    </div>
                    <!-- /.row -->
                    <!-- S1T2 -->
                    <div class="row">
                        <div class="col-lg-12">
                            <h5 id="s1t2">Step 1. Trial 2:</h5>
                            <p>Compute the polarity score of individual headlines</p>
                            <div class="panel panel-default">
                                <div class="panel-body" style="overflow: scroll">
                                    <iframe width="10000" height="400" frameborder="0" scrolling="no" src="//plot.ly/~valem/36.embed"></iframe>
                                </div>
                                <div class="panel-footer">
                                    <div class="small text-center">Partial view of polarity scores for top 25 individual Reddit World News headlines from 8/8/2008 to 7/1/2016.
                                        <p><strong>Features:</strong> date, and positive, negative, neutral and compound scores for each of the top 25 headlines on the given date <strong>Dimension:</strong> 1990 x 101
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <!-- /.row -->
                    <div class="row">
                        <div class="col-lg-12">
                            <div class="well">
                                <h5>Description of Changes</h5>
                                <p>Compute four scores (positive, negative, neutral, compound) for each headline, then average the scores across all top 25 headlines within each polarity category.</p>
                            </div>
                            <div class="well">
                                <h5>Motivation</h5>
                                <p>In our previous computation, all compound scores approximated -1 because on average there were more negative words in the combined headlines, so the absolute value of negative scores was always larger than that of positive scores. The Vader module is dependent on the number of words in the sentence, therefore it will produce more accurate results if we consider each headline to be its own sentence. We keep polarity scores separated into four categories (positive, negative, neutral, compound) because these features will be considered when training the prediction model.</p>
                            </div>
                        </div>
                    </div>
                    <!-- /.row -->
                    <div class="row">
                        <div class="col-lg-12">
                            <div class="panel panel-default">
                                <div class="panel-body" style="overflow: scroll">
                                    <iframe width="900" height="400" frameborder="0" scrolling="no" src="//plot.ly/~valem/38.embed"></iframe>
                                </div>
                                <div class="panel-footer">
                                    <div class="small text-center">Partial view of average polarity scores for the sum of top 25 individual Reddit World News headlines from 8/8/2008 to 7/1/2016. All headlines are equally weighted.
                                        <p><strong>Features:</strong> date, positive, negative, neutral and compound average scores across the top 25 headlines on the given date <strong>Dimension:</strong> 1990 x 5
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <!-- /.row -->
                    <br>
                    <!-- Step 2 -->
                    <div class="row">
                        <div class="col-lg-12">
                            <h5 id="s2t1">Step 2.</h5>
                            <p>Train the logistic regression model</p>
                            <div class="well">
                                <h5>Description of Changes</h5>
                                <p>Compute four scores (positive, negative, neutral, compound) for each headline, then average the scores across all top 25 headlines within each polarity category.</p>
                            </div>
                        </div>
                    </div>
                    <div class="row">
                        <div class="col-lg-12">
                            <div class="panel panel-default">
                                <div class="panel-body" style="overflow: scroll">
                                    plotly iframe
                                </div>
                                <div class="panel-footer">
                                    <div class="small text-center">Partial view of polarity scores for top 25 individual Reddit World News headlines from 8/8/2008 to 7/1/2016.
                                        <p><strong>Features:</strong> date, and positive, negative, neutral and compound scores for each of the top 25 headlines on the given date <strong>Dimension:</strong> 1990 x 101
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <!-- /.row -->
                </div>
                <!-- /#analysis-pills -->
                <div class="tab-pane" id="results-pills">
                </div>
                <!-- /#results-pills -->
                <div class="tab-pane" id="discussion-pills">
                    <div class="row gg-row-truncate">
                        <div class="col-lg-12">
                            <h4>Discussion</h4>
                            <p>The accuracy rate of previous logistic model is about 0.52, which is not obvious higher than throwing a coin. So far we can not draw our conclusion that the news data has no effect on stock movement. And there are still many ways to improve the accuracy rate in next period work.</p>
                            <p>First, we can change our datasets. Now we are using a worldwide and not financial-oriented news dataset to promote the DJIA movement. We can put new datasets which are more strongly related to financial news in America. Meanwhile, we can add other stock markets’ price dataset to our data collection.</p>
                            <p>Second, we may use other nlp (natural language processing) algorithm, and use finance corpus to train the model. There are also many algorithms involving deep learning field, such as LSTM (long-short term method), that may fit this case. 
                            </p>
                            <p>What’s more, the missing value of stock data (there are not data in Saturday and Sunday for the market is closed) should be fully considered. And we may use weekly value as a unit value to avoid the missing value problem.</p>
                        </div>
                    </div>
                </div>
                <!-- /#discussion-pills -->
            </div>
            <!-- /.tab-content -->
            <footer class="footer">
                <div class="container">
                    <div class="row">
                        <div class="col-lg-12">
                            <p class="text-center small">
                                <a href="javascript:" id="gg-to-top">
                                    <i class="fa fa-chevron-up fa-fw"></i>
                                </a>
                                <br>Mack, D., Pan, C., Siqi, D. & Xia, Z., 2016</p>
                        </div>
                    </div>
                </div>
            </footer>
            <!-- Footer
            <footer class="footer">
                <div class="container">
                    <div class="row">
                        <div class="col-lg-12">
                            <h4>Information</h4>
                            <div class="well">
                                <p>These datasets were used...</p>
                            </div>
                            <div class="well" id="">
                                <p>different</p>
                            </div>
                        </div>
                    </div>
                </div>
            </footer>
             /.footer -->
        </div>
        <!-- /#page-wrapper -->
    </div>
    <!-- /#wrapper -->

    <!-- jQuery -->
    <script src="../vendor/jquery/jquery.min.js"></script>
    
    <!-- Custom -->
    <script src="../js/script.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="../vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Metis Menu Plugin JavaScript -->
    <script src="../vendor/metisMenu/metisMenu.min.js"></script>

    <!-- Morris Charts JavaScript -->
    <script src="../vendor/raphael/raphael.min.js"></script>
    <script src="../vendor/morrisjs/morris.min.js"></script>
    <script src="../data/morris-data.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="../dist/js/sb-admin-2.js"></script>

</body>

</html>
